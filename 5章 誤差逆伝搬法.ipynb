{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 計算グラフ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mnist import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 値をエッジの重み，演算をノードとしたグラフを計算グラフと呼ぶ\n",
    "- 順方向には通常の計算，逆方向には微分になる.\n",
    "- 部分的な式を微分して外側の式に掛けていく，合成関数の微分をグラフ表現したもの\n",
    "- 加算ノードの逆方向はそのまま計算\n",
    "- 乗算ノードの逆方向は元の式からその変数を抜いたものを計算．xyなら，x方向にはy*入力って感じ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "乗算レイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220.00000000000003"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.2, 200, 110.00000000000001)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "#layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "price\n",
    "\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "print(dapple_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "dapple, dtax, dapple_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$price = apple \\times apple\\_num \\times tax$$\n",
    "のとき,  \n",
    "逆伝播の結果$dapple, dtax, dapple\\_num$は，  \n",
    "$apple, tax, apple\\_num$がそれぞれ$dprice=1$だけ増えた時，$price$がどれだけ変化するかを表している．  \n",
    "\n",
    "たとえば，りんごの価格$apple$が1円だけ増えた時，priceは2.2円だけ増加する．  \n",
    "すなわちこれは，りんごの価格の変数$apple$方向の傾きを求めていることになる．  \n",
    "したがって，逆伝播は関数を微分して実際の値を入れることに相当する．  \n",
    "\n",
    "例えば，$a=xyz$なら，$x$の逆伝播の結果は$a$方向の傾きとなる関数$a'=yz$の点$(x,y,z)$における傾きということになる．  \n",
    "ここで$x$軸方向の傾きの大きさは$x$によらないことがわかる． \n",
    "\n",
    "計算グラフの逆伝播操作は，各エッジ(値)の勾配を求めているということになる．  \n",
    "計算グラフの尻から誤差を与えてやると，誤差が大きいほど各エッジの勾配の値も大きくなる．  \n",
    "この勾配に学習率をかけて元のエッジの値を傾きが小さくなる方向へ更新することを繰り返すのが，勾配降下法による学習である．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加算レイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "715.0000000000001"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1.1, 650, 1.1, 1.1, 2.2, 110.00000000000001, 3.3000000000000003, 165.0)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "apple_orange_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(apple_orange_price, tax)\n",
    "\n",
    "price\n",
    "\n",
    "dprice = 1\n",
    "apple_orange_price_d, tax_d = mul_tax_layer.backward(dprice)\n",
    "apple_price_d, orange_price_d = add_apple_orange_layer.backward(apple_orange_price_d)\n",
    "apple_d, apple_num_d = mul_apple_layer.backward(apple_price_d)\n",
    "orange_d, orange_num_d = mul_orange_layer.backward(orange_price_d)\n",
    "\n",
    "apple_orange_price_d, tax_d, apple_price_d, orange_price_d, apple_d, apple_num_d, orange_d, orange_num_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 活性化関数レイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLUレイヤの実装  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rctified Linear Unit  \n",
    "入力xが正ならそのまま，負なら0を返す関数  \n",
    "微分すると，それぞれ1, 0となる  \n",
    "順伝搬時0なら下流への信号がストップするということ  \n",
    "電気回路におけるスイッチのように機能する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 3.]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 3.]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "\n",
    "relu = Relu()\n",
    "relu_out = relu.forward(x)\n",
    "relu_out\n",
    "\n",
    "relu_out_d = relu.backward(relu_out)\n",
    "relu_out_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoidレイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = \\frac{1}{1 + exp(-x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "順に計算グラフにするとx*-1(exp)+1/となる．  \n",
    "1ステップずつ逆伝搬させると"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial L}{\\partial y}y^2 exp(-x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これくらいのノードはグループ化して入出力だけ考えたほうがわかりやすく，効率もいい  \n",
    "上記の偏微分の式にシグモイド関数の定義式を代入すると"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial L}{\\partial y} y(1-y) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = 1 / 1 + np.exp(-x)\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine / Softmaxレイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine レイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1.91845902, 1.27053967, 1.16922066])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.rand(2)\n",
    "W = np.random.rand(2, 3)\n",
    "B = np.random.rand(3)\n",
    "\n",
    "X.shape\n",
    "W.shape\n",
    "B.shape\n",
    "\n",
    "Y = np.dot(X, W) + B\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "内積も扱うレイヤ  \n",
    "$$ L = X \\cdot W + B $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチ版Affineレイヤ  \n",
    "1行のベクトルではなく，N行に対応する  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バイアスの逆伝搬の時は，N行のベクトルをタテに和を取る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dY = np.array([[1,2,3], [4,5,6]]) # N=2のとき\n",
    "dB = np.sum(dY, axis=0) # Axis設定しないとflattenされて和をとられる\n",
    "dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax-with-Lossレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exp(x)の和でexp(x)を割って確率にする層  \n",
    "Softmaxに通す前の値をスコアと呼ぶ  \n",
    "推論フェーズではSoftmaxレイヤは使われない  \n",
    "推論フェーズでは最大値だけわかればいい  \n",
    "学習フェーズでは他の奴と比べてどうか？が欲しいため，Softmaxが必要になる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クロスエントロピー誤差も入れる  \n",
    "実装を簡単化できるが，過程は付録A  \n",
    "すると，この出力層の微分は，出力-教師データ と，誤差を後ろに流すことになる．  \n",
    "なお，恒等関数＋2乗和誤差も微分の結果はその誤差になる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef softmax(a):\\n    c = np.max(a)\\n    exp_a = np.exp(a-c)\\n    exp_a_sum = np.sum(exp_a)\\n    return exp_a / exp_a_sum\\n\\ndef cross_entropy_error(y, t):\\n    delta = 1e-7\\n    return -np.sum(t * np.log(y + delta))\\n'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a-c)\n",
    "    exp_a_sum = np.sum(exp_a)\n",
    "    return exp_a / exp_a_sum\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\"\"\"\n",
    "\n",
    "# このsoftmaxとcrossentropyerrorはgithubに上がってる方のコードでないといけない\n",
    "\n",
    "def softmax(x):\n",
    "    # checkで使われるのはこっち\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "    \n",
    "    # 従来のsoftmax\n",
    "    x = x - np.max(x) # オーバーフロー対策\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    # softmax + cross entropy errorの逆伝播はy - tになる\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size # n個のデータの誤差を見たので，Nで割る\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 誤差逆伝搬法の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ミニバッチを取ってくる\n",
    "- 損失関数で求めた値から，勾配を求める\n",
    "- パラメータを更新する\n",
    "- 上記ステップを繰り返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient_for_a_vector(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = float(tmp_val) - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "        x[idx] = tmp_val\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def numerical_gradient(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return numerical_gradient_for_a_vector(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = numerical_gradient_for_a_vector(f, x)\n",
    "        \n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        # 追加部分\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    \n",
    "    # Xは何個でも画像をもらえる．それぞれの画像に対する予測結果を返す．\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1) # デフォルトは縦でargmaxを求めてしまう．1行の中の最大値が推測結果．\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # 数値微分による勾配の計算，遅い．何度も「ちょっと重みを変えてpredict」を行う．\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads[\"W1\"] = numerical_gradient(loss_W, self.params[\"W1\"])\n",
    "        grads[\"b1\"] = numerical_gradient(loss_W, self.params[\"b1\"])\n",
    "        grads[\"W2\"] = numerical_gradient(loss_W, self.params[\"W2\"])\n",
    "        grads[\"b2\"] = numerical_gradient(loss_W, self.params[\"b2\"])\n",
    "        \n",
    "        return grads\n",
    "\n",
    "    # 誤差逆伝搬法による勾配の計算，早い．1回のpredictでいい．\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        #backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 : 3.885315542475368e-10\n",
      "b1 : 2.476091421996663e-09\n",
      "W2 : 5.377092578844242e-09\n",
      "b2 : 1.4005354904977896e-07\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key, \":\", str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 誤差逆伝搬法を使った学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22b006daf98>]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22b00657be0>]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # ここが4章とは違う！誤差逆伝搬法を使っている\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', \"b2\"):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        # print(train_acc, test_acc)\n",
    "\n",
    "X1 = np.arange(0, len(train_acc_list))\n",
    "X2 = np.arange(0, len(test_acc_list))\n",
    "plt.plot(X1, train_acc_list)\n",
    "plt.plot(X2, test_acc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "めちゃはやい！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まとめ  \n",
    "勾配が求まったら，その全てのベクトルに学習率をかけてパラメータを更新する．この勾配を求めるのを高速化したかった．  \n",
    "勾配を数値微分で求める＝＞パラメータすべてに対して，少し変えてlossを求める，を繰り返すので，めちゃ遅い  \n",
    "勾配を誤差逆伝搬法で求める＝＞計算グラフの考え方で，後ろの層の微小変化に対し，行列計算で一気に勾配ベクトル(微分)を求められる  \n",
    "\n",
    "1を出力側から与えて計算グラフを次の規則に従って逆に計算していくと，各辺(値)の微分値が求まる．  \n",
    "乗算レイヤ：xyzなら，xにはyzが逆伝搬   \n",
    "加算レイヤ：xyzなら，xにはxが逆伝搬  \n",
    "ReLuレイヤ：入力が正なら1，そうでなければ0を逆伝搬  \n",
    "Sigmoidレイヤ：$$ \\frac{\\partial L}{\\partial y} y(1-y) $$ が逆伝搬，偏微分のところは，出力側から与えられる微小変化  \n",
    "Affineレイヤ：入力ベクトルに重みとバイアスを与えるレイヤ，重みとバイアスの辺に，それぞれベクトルと値を行列の積によって返す  \n",
    "Softmax with Lossレイヤ：出力と正解の差が逆伝搬するよう設計された層．  \n",
    "恒等関数には２乗誤差，Softmaxにはクロスエントロピーを使うことで，ただその差が逆伝搬する  \n",
    "\n",
    "詳しくは，付録Aやディープラーニングがわかる数学入門で！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
